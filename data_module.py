# data_module.py - Project Odysseus ë°ì´í„° ìˆ˜ì§‘ ë° ê´€ë¦¬ ëª¨ë“ˆ
# ë™ì  ì ì‘í˜• í˜ì–´ íŠ¸ë ˆì´ë”© ë´‡ì˜ í•µì‹¬ ë°ì´í„° íŒŒì´í”„ë¼ì¸

import asyncio
import time
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Tuple, Any
from dataclasses import dataclass
from enum import Enum
import ccxt
import psycopg2
from psycopg2.extras import RealDictCursor
from sqlalchemy import create_engine, text
from sqlalchemy.exc import SQLAlchemyError
import logging
from loguru import logger
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# Project Odysseus ì„¤ì • import
from config import settings, get_db_url, get_exchange_config

# =============================================================================
# 1. ë°ì´í„° ëª¨ë¸ ë° ì—´ê±°í˜•
# =============================================================================

class DataStatus(Enum):
    """ë°ì´í„° ìƒíƒœ"""
    VALID = "valid"
    INTERPOLATED = "interpolated" 
    MISSING = "missing"
    CORRUPTED = "corrupted"

class DataSource(Enum):
    """ë°ì´í„° ì†ŒìŠ¤"""
    API = "api"
    WEBSOCKET = "websocket"
    MANUAL = "manual"
    INTERPOLATION = "interpolation"

@dataclass
class MarketData:
    """ì‹œì¥ ë°ì´í„° êµ¬ì¡°ì²´"""
    symbol: str
    timeframe: str
    timestamp: datetime
    open: float
    high: float
    low: float
    close: float
    volume: float
    quote_volume: Optional[float] = None
    trades_count: Optional[int] = None
    taker_buy_volume: Optional[float] = None
    taker_buy_quote_volume: Optional[float] = None
    data_source: DataSource = DataSource.API
    is_interpolated: bool = False
    
    def __post_init__(self):
        """ë°ì´í„° ê²€ì¦"""
        if not all([self.open > 0, self.high > 0, self.low > 0, self.close > 0]):
            raise ValueError(f"Invalid OHLC data for {self.symbol}: O={self.open}, H={self.high}, L={self.low}, C={self.close}")
        
        if not (self.high >= max(self.open, self.close) and self.low <= min(self.open, self.close)):
            raise ValueError(f"OHLC logic violation for {self.symbol}")
        
        if self.volume < 0:
            raise ValueError(f"Negative volume for {self.symbol}: {self.volume}")

@dataclass
class DataQualityMetrics:
    """ë°ì´í„° í’ˆì§ˆ ë©”íŠ¸ë¦­"""
    symbol: str
    total_records: int
    missing_records: int
    interpolated_records: int
    corrupted_records: int
    quality_score: float
    last_update: datetime
    
    @property
    def missing_ratio(self) -> float:
        return self.missing_records / max(self.total_records, 1)
    
    @property
    def is_healthy(self) -> bool:
        return self.quality_score >= 0.95 and self.missing_ratio <= 0.05

# =============================================================================
# 2. ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ê´€ë¦¬ì
# =============================================================================

class DatabaseManager:
    """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ë° ì‘ì—… ê´€ë¦¬"""
    
    def __init__(self):
        self.db_url = get_db_url()
        self.engine = create_engine(
            self.db_url,
            pool_size=10,
            max_overflow=20,
            pool_pre_ping=True,
            pool_recycle=3600  # 1ì‹œê°„ë§ˆë‹¤ ì—°ê²° ì¬ìƒì„±
        )
        self.connection = None
        
    def __enter__(self):
        """ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§„ì…"""
        self.connection = self.engine.connect()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ"""
        if self.connection:
            self.connection.close()
    
    def test_connection(self) -> bool:
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸"""
        try:
            with self.engine.connect() as conn:
                result = conn.execute(text("SELECT 1"))
                return result.scalar() == 1
        except Exception as e:
            logger.error(f"Database connection test failed: {e}")
            return False
    
    def execute_query(self, query: str, params: Dict = None) -> Any:
        """ì¿¼ë¦¬ ì‹¤í–‰"""
        try:
            if self.connection:
                return self.connection.execute(text(query), params or {})
            else:
                with self.engine.connect() as conn:
                    return conn.execute(text(query), params or {})
        except SQLAlchemyError as e:
            logger.error(f"Database query failed: {query[:100]}... Error: {e}")
            raise
    
    def insert_market_data(self, data_list: List[MarketData]) -> int:
        """ì‹œì¥ ë°ì´í„° ëŒ€ëŸ‰ ì‚½ì…"""
        if not data_list:
            return 0
        
        insert_query = """
            INSERT INTO market_data.price_data 
            (time, symbol, exchange, timeframe, open, high, low, close, volume,
             quote_volume, trades_count, taker_buy_volume, taker_buy_quote_volume,
             is_interpolated, data_source, created_at)
            VALUES 
            (%(timestamp)s, %(symbol)s, %(exchange)s, %(timeframe)s, 
             %(open)s, %(high)s, %(low)s, %(close)s, %(volume)s,
             %(quote_volume)s, %(trades_count)s, %(taker_buy_volume)s, %(taker_buy_quote_volume)s,
             %(is_interpolated)s, %(data_source)s, NOW())
            ON CONFLICT (time, symbol, exchange, timeframe) 
            DO UPDATE SET
                open = EXCLUDED.open,
                high = EXCLUDED.high, 
                low = EXCLUDED.low,
                close = EXCLUDED.close,
                volume = EXCLUDED.volume,
                quote_volume = EXCLUDED.quote_volume,
                is_interpolated = EXCLUDED.is_interpolated,
                data_source = EXCLUDED.data_source
        """
        
        records = []
        for data in data_list:
            records.append({
                'timestamp': data.timestamp,
                'symbol': data.symbol,
                'exchange': 'binance',  # í˜„ì¬ëŠ” ë°”ì´ë‚¸ìŠ¤ë§Œ ì§€ì›
                'timeframe': data.timeframe,
                'open': data.open,
                'high': data.high,
                'low': data.low,
                'close': data.close,
                'volume': data.volume,
                'quote_volume': data.quote_volume,
                'trades_count': data.trades_count,
                'taker_buy_volume': data.taker_buy_volume,
                'taker_buy_quote_volume': data.taker_buy_quote_volume,
                'is_interpolated': data.is_interpolated,
                'data_source': data.data_source.value
            })
        
        try:
            with self.engine.connect() as conn:
                result = conn.execute(text(insert_query), records)
                conn.commit()
                return len(records)
        except SQLAlchemyError as e:
            logger.error(f"Failed to insert market data: {e}")
            raise

# =============================================================================
# 3. ê±°ë˜ì†Œ API í´ë¼ì´ì–¸íŠ¸
# =============================================================================

class ExchangeAPIManager:
    """ê±°ë˜ì†Œ API í†µì‹  ê´€ë¦¬ì"""
    
    def __init__(self):
        self.exchange_config = get_exchange_config()
        self.exchange = self._initialize_exchange()
        self.session = self._create_session()
        
        # ì¬ì‹œë„ ì„¤ì • (ì§€ìˆ˜ ë°±ì˜¤í”„)
        self.max_retries = 5
        self.base_delay = 1.0
        self.max_delay = 60.0
        
    def _initialize_exchange(self) -> ccxt.Exchange:
        """ê±°ë˜ì†Œ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        try:
            exchange_class = getattr(ccxt, settings.exchanges.primary_exchange.value)
            exchange = exchange_class(self.exchange_config)
            
            # API ì—°ê²° í…ŒìŠ¤íŠ¸
            exchange.load_markets()
            logger.info(f"âœ… {settings.exchanges.primary_exchange.value} API initialized successfully")
            
            return exchange
        except Exception as e:
            logger.error(f"âŒ Failed to initialize exchange API: {e}")
            raise
    
    def _create_session(self) -> requests.Session:
        """HTTP ì„¸ì…˜ ìƒì„± (ì¬ì‹œë„ ì •ì±… í¬í•¨)"""
        session = requests.Session()
        
        # ì¬ì‹œë„ ì „ëµ ì„¤ì •
        retry_strategy = Retry(
            total=3,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["HEAD", "GET", "OPTIONS"]
        )
        
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("http://", adapter)
        session.mount("https://", adapter)
        
        return session
    
    async def fetch_ohlcv_with_retry(
        self, 
        symbol: str, 
        timeframe: str, 
        since: Optional[int] = None,
        limit: Optional[int] = None
    ) -> List[List]:
        """OHLCV ë°ì´í„° ì¡°íšŒ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)"""
        
        for attempt in range(self.max_retries):
            try:
                logger.debug(f"Fetching OHLCV for {symbol} {timeframe}, attempt {attempt + 1}")
                
                # ccxtë¥¼ í†µí•œ ë°ì´í„° ì¡°íšŒ
                ohlcv_data = self.exchange.fetch_ohlcv(
                    symbol=symbol,
                    timeframe=timeframe,
                    since=since,
                    limit=limit
                )
                
                if not ohlcv_data:
                    raise ValueError(f"No data returned for {symbol} {timeframe}")
                
                logger.debug(f"âœ… Successfully fetched {len(ohlcv_data)} records for {symbol}")
                return ohlcv_data
                
            except ccxt.NetworkError as e:
                logger.warning(f"ğŸŒ Network error for {symbol} (attempt {attempt + 1}): {e}")
                if attempt < self.max_retries - 1:
                    delay = min(self.base_delay * (2 ** attempt), self.max_delay)
                    await asyncio.sleep(delay)
                    continue
                else:
                    logger.error(f"âŒ Network error persists after {self.max_retries} attempts")
                    raise
                    
            except ccxt.ExchangeError as e:
                # API í•œë„ ì´ˆê³¼ ë“±ì˜ ê±°ë˜ì†Œ ì˜¤ë¥˜
                if "rate limit" in str(e).lower():
                    logger.warning(f"â±ï¸ Rate limit hit for {symbol}, waiting...")
                    delay = min(self.base_delay * (2 ** attempt), self.max_delay)
                    await asyncio.sleep(delay)
                    continue
                else:
                    logger.error(f"âŒ Exchange error for {symbol}: {e}")
                    raise
                    
            except Exception as e:
                logger.error(f"âŒ Unexpected error fetching {symbol}: {e}")
                if attempt < self.max_retries - 1:
                    delay = min(self.base_delay * (2 ** attempt), self.max_delay)
                    await asyncio.sleep(delay)
                    continue
                else:
                    raise
        
        # ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨
        raise Exception(f"Failed to fetch {symbol} {timeframe} after {self.max_retries} attempts")
    
    def convert_ohlcv_to_market_data(
        self, 
        ohlcv_list: List[List], 
        symbol: str, 
        timeframe: str
    ) -> List[MarketData]:
        """OHLCV ì›ì‹œ ë°ì´í„°ë¥¼ MarketData ê°ì²´ë¡œ ë³€í™˜"""
        market_data_list = []
        
        for ohlcv in ohlcv_list:
            try:
                # ccxt OHLCV í˜•ì‹: [timestamp, open, high, low, close, volume]
                timestamp = datetime.fromtimestamp(ohlcv[0] / 1000)  # ms to seconds
                
                market_data = MarketData(
                    symbol=symbol,
                    timeframe=timeframe,
                    timestamp=timestamp,
                    open=float(ohlcv[1]),
                    high=float(ohlcv[2]),
                    low=float(ohlcv[3]),
                    close=float(ohlcv[4]),
                    volume=float(ohlcv[5]),
                    data_source=DataSource.API
                )
                
                market_data_list.append(market_data)
                
            except (ValueError, IndexError) as e:
                logger.warning(f"âš ï¸ Invalid OHLCV data for {symbol}: {ohlcv}, Error: {e}")
                continue
        
        return market_data_list

# =============================================================================
# 4. ë°ì´í„° ê²€ì¦ ë° í’ˆì§ˆ ê´€ë¦¬ì
# =============================================================================

class DataValidator:
    """ë°ì´í„° ê²€ì¦ ë° í’ˆì§ˆ ê´€ë¦¬"""
    
    def __init__(self):
        self.validation_policy = settings.data_collection.validation_policy
        self.max_interpolation_gap = 5  # ìµœëŒ€ 5ê°œê¹Œì§€ë§Œ ë³´ê°„
        
    def validate_data_integrity(self, data_list: List[MarketData]) -> Tuple[List[MarketData], DataQualityMetrics]:
        """ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦ ë° í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        
        if not data_list:
            return [], DataQualityMetrics(
                symbol="unknown",
                total_records=0,
                missing_records=0,
                interpolated_records=0,
                corrupted_records=0,
                quality_score=0.0,
                last_update=datetime.now()
            )
        
        symbol = data_list[0].symbol
        original_count = len(data_list)
        validated_data = []
        corrupted_count = 0
        
        # 1. ê¸°ë³¸ ë°ì´í„° ê²€ì¦
        for data in data_list:
            try:
                # MarketData ìì²´ ê²€ì¦ (__post_init__)ì´ ì‹¤í–‰ë¨
                validated_data.append(data)
            except ValueError as e:
                logger.warning(f"âš ï¸ Corrupted data detected for {symbol}: {e}")
                corrupted_count += 1
        
        # 2. ì‹œê°„ ìˆœì„œ ì •ë ¬
        validated_data.sort(key=lambda x: x.timestamp)
        
        # 3. ëˆ„ë½ ë°ì´í„° íƒì§€ ë° ë³´ê°„
        interpolated_data, interpolated_count = self._handle_missing_data(validated_data)
        
        # 4. í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        quality_score = self._calculate_quality_score(
            total=original_count,
            corrupted=corrupted_count,
            interpolated=interpolated_count
        )
        
        # 5. í’ˆì§ˆ ë©”íŠ¸ë¦­ ìƒì„±
        quality_metrics = DataQualityMetrics(
            symbol=symbol,
            total_records=len(interpolated_data),
            missing_records=max(0, original_count - len(validated_data)),
            interpolated_records=interpolated_count,
            corrupted_records=corrupted_count,
            quality_score=quality_score,
            last_update=datetime.now()
        )
        
        logger.info(f"ğŸ“Š Data validation completed for {symbol}: "
                   f"Quality={quality_score:.3f}, "
                   f"Total={len(interpolated_data)}, "
                   f"Interpolated={interpolated_count}, "
                   f"Corrupted={corrupted_count}")
        
        return interpolated_data, quality_metrics
    
    def _handle_missing_data(self, data_list: List[MarketData]) -> Tuple[List[MarketData], int]:
        """ëˆ„ë½ ë°ì´í„° ì²˜ë¦¬ (ë³´ê°„ ë˜ëŠ” ì œì™¸)"""
        
        if len(data_list) < 2:
            return data_list, 0
        
        # ì‹œê°„ ê°„ê²© ê³„ì‚° (timeframe ê¸°ì¤€)
        timeframe = data_list[0].timeframe
        expected_interval = self._get_timeframe_minutes(timeframe)
        
        filled_data = []
        interpolated_count = 0
        
        for i in range(len(data_list)):
            filled_data.append(data_list[i])
            
            # ë‹¤ìŒ ë°ì´í„°ì™€ì˜ ì‹œê°„ ê°„ê²© í™•ì¸
            if i < len(data_list) - 1:
                current_time = data_list[i].timestamp
                next_time = data_list[i + 1].timestamp
                time_diff_minutes = (next_time - current_time).total_seconds() / 60
                
                expected_gaps = int(time_diff_minutes / expected_interval) - 1
                
                # ëˆ„ë½ëœ ë°ì´í„°ê°€ ìˆê³ , ë³´ê°„ ê°€ëŠ¥í•œ ë²”ìœ„ ë‚´ì¸ ê²½ìš°
                if expected_gaps > 0 and expected_gaps <= self.max_interpolation_gap:
                    if self.validation_policy == settings.data_collection.DataValidationPolicy.INTERPOLATE:
                        interpolated_points = self._interpolate_data(
                            data_list[i], 
                            data_list[i + 1], 
                            expected_gaps
                        )
                        filled_data.extend(interpolated_points)
                        interpolated_count += len(interpolated_points)
                        
                        logger.debug(f"ğŸ”§ Interpolated {len(interpolated_points)} points between "
                                   f"{current_time} and {next_time} for {data_list[i].symbol}")
                
                elif expected_gaps > self.max_interpolation_gap:
                    # ë„ˆë¬´ í° ê°„ê²©ì€ ì‹¬ê°í•œ ë¬¸ì œë¡œ ê°„ì£¼
                    logger.error(f"âŒ Large data gap detected for {data_list[i].symbol}: "
                               f"{expected_gaps} missing intervals between {current_time} and {next_time}")
        
        return filled_data, interpolated_count
    
    def _interpolate_data(self, start_data: MarketData, end_data: MarketData, gap_count: int) -> List[MarketData]:
        """ì„ í˜• ë³´ê°„ìœ¼ë¡œ ëˆ„ë½ ë°ì´í„° ìƒì„±"""
        interpolated = []
        
        # ì‹œê°„ ê°„ê²© ê³„ì‚°
        time_delta = (end_data.timestamp - start_data.timestamp) / (gap_count + 1)
        
        for i in range(1, gap_count + 1):
            # ì„ í˜• ë³´ê°„ ë¹„ìœ¨
            ratio = i / (gap_count + 1)
            
            interpolated_timestamp = start_data.timestamp + (time_delta * i)
            
            # OHLC ë°ì´í„° ì„ í˜• ë³´ê°„
            interpolated_data = MarketData(
                symbol=start_data.symbol,
                timeframe=start_data.timeframe,
                timestamp=interpolated_timestamp,
                open=start_data.close + (end_data.open - start_data.close) * ratio,
                high=max(start_data.close, end_data.open) + abs(start_data.high - end_data.high) * ratio,
                low=min(start_data.close, end_data.open) - abs(start_data.low - end_data.low) * ratio,
                close=start_data.close + (end_data.close - start_data.close) * ratio,
                volume=start_data.volume + (end_data.volume - start_data.volume) * ratio,
                data_source=DataSource.INTERPOLATION,
                is_interpolated=True
            )
            
            interpolated.append(interpolated_data)
        
        return interpolated
    
    def _get_timeframe_minutes(self, timeframe: str) -> int:
        """timeframe ë¬¸ìì—´ì„ ë¶„ ë‹¨ìœ„ë¡œ ë³€í™˜"""
        timeframe_minutes = {
            '1m': 1,
            '5m': 5,
            '15m': 15,
            '30m': 30,
            '1h': 60,
            '2h': 120,
            '4h': 240,
            '6h': 360,
            '12h': 720,
            '1d': 1440
        }
        return timeframe_minutes.get(timeframe, 60)  # ê¸°ë³¸ê°’: 1ì‹œê°„
    
    def _calculate_quality_score(self, total: int, corrupted: int, interpolated: int) -> float:
        """ë°ì´í„° í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (0.0 ~ 1.0)"""
        if total == 0:
            return 0.0
        
        # ê°€ì¤‘ì¹˜: ì†ìƒëœ ë°ì´í„°ëŠ” ë³´ê°„ëœ ë°ì´í„°ë³´ë‹¤ ë” ì‹¬ê°í•œ ë¬¸ì œ
        corruption_penalty = (corrupted * 0.5) / total
        interpolation_penalty = (interpolated * 0.2) / total
        
        quality_score = max(0.0, 1.0 - corruption_penalty - interpolation_penalty)
        return min(1.0, quality_score)

# =============================================================================
# 5. ë©”ì¸ ë°ì´í„° í•¸ë“¤ëŸ¬ í´ë˜ìŠ¤
# =============================================================================

class DataHandler:
    """Project Odysseus ë©”ì¸ ë°ì´í„° ìˆ˜ì§‘ ë° ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        """ë°ì´í„° í•¸ë“¤ëŸ¬ ì´ˆê¸°í™”"""
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.db_manager = DatabaseManager()
        self.api_manager = ExchangeAPIManager()
        self.validator = DataValidator()
        
        # ì„¤ì •ê°’ ë¡œë“œ
        self.target_symbols = ['BTC/USDT', 'ETH/USDT', 'BNB/USDT', 'ADA/USDT', 'DOT/USDT']
        self.primary_timeframe = '1h'
        self.collection_interval = 60  # ê°œë°œ ë‹¨ê³„: 60ì´ˆ
        
        # í’ˆì§ˆ ë©”íŠ¸ë¦­ ì¶”ì 
        self.quality_metrics: Dict[str, DataQualityMetrics] = {}
        
        # ì‹¤í–‰ ìƒíƒœ
        self.is_running = False
        self.last_collection_time = None
        
        logger.info("ğŸš€ DataHandler initialized successfully")
        logger.info(f"ğŸ“Š Target symbols: {self.target_symbols}")
        logger.info(f"â±ï¸ Collection interval: {self.collection_interval}s")
        logger.info(f"ğŸ• Primary timeframe: {self.primary_timeframe}")
    
    def test_connections(self) -> bool:
        """ëª¨ë“  ì—°ê²° í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸ” Testing all connections...")
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸
        if not self.db_manager.test_connection():
            logger.error("âŒ Database connection failed")
            return False
        logger.info("âœ… Database connection OK")
        
        # ê±°ë˜ì†Œ API í…ŒìŠ¤íŠ¸
        try:
            markets = self.api_manager.exchange.load_markets()
            if not markets:
                logger.error("âŒ Exchange API test failed: No markets loaded")
                return False
            logger.info(f"âœ… Exchange API OK ({len(markets)} markets loaded)")
        except Exception as e:
            logger.error(f"âŒ Exchange API test failed: {e}")
            return False
        
        return True
    
    async def fetch_historical_data(
        self, 
        symbol: str, 
        timeframe: str = None,
        days_back: int = 30
    ) -> List[MarketData]:
        """ê³¼ê±° ë°ì´í„° ìˆ˜ì§‘"""
        
        timeframe = timeframe or self.primary_timeframe
        since_timestamp = int((datetime.now() - timedelta(days=days_back)).timestamp() * 1000)
        
        logger.info(f"ğŸ“š Fetching historical data for {symbol} {timeframe} "
                   f"(last {days_back} days)")
        
        try:
            # APIì—ì„œ ë°ì´í„° ì¡°íšŒ
            ohlcv_data = await self.api_manager.fetch_ohlcv_with_retry(
                symbol=symbol,
                timeframe=timeframe,
                since=since_timestamp,
                limit=1000  # ëŒ€ë¶€ë¶„ ê±°ë˜ì†Œì˜ ìµœëŒ€ ì œí•œ
            )
            
            # MarketData ê°ì²´ë¡œ ë³€í™˜
            market_data_list = self.api_manager.convert_ohlcv_to_market_data(
                ohlcv_data, symbol, timeframe
            )
            
            # ë°ì´í„° ê²€ì¦ ë° í’ˆì§ˆ ê´€ë¦¬
            validated_data, quality_metrics = self.validator.validate_data_integrity(market_data_list)
            self.quality_metrics[symbol] = quality_metrics
            
            logger.info(f"âœ… Historical data collected for {symbol}: "
                       f"{len(validated_data)} records, quality={quality_metrics.quality_score:.3f}")
            
            return validated_data
            
        except Exception as e:
            logger.error(f"âŒ Failed to fetch historical data for {symbol}: {e}")
            raise
    
    async def fetch_realtime_data(self, symbol: str, timeframe: str = None) -> List[MarketData]:
        """ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘ (ìµœì‹  ëª‡ ê°œ ìº”ë“¤)"""
        
        timeframe = timeframe or self.primary_timeframe
        
        try:
            # ìµœê·¼ 5ê°œ ìº”ë“¤ ì¡°íšŒ (í™•ì‹¤í•œ ì™„ë£Œëœ ìº”ë“¤ì„ ìœ„í•´)
            ohlcv_data = await self.api_manager.fetch_ohlcv_with_retry(
                symbol=symbol,
                timeframe=timeframe,
                limit=5
            )
            
            # MarketData ê°ì²´ë¡œ ë³€í™˜
            market_data_list = self.api_manager.convert_ohlcv_to_market_data(
                ohlcv_data, symbol, timeframe
            )
            
            # ìµœì‹  ë°ì´í„°ë§Œ ë°˜í™˜ (ë§ˆì§€ë§‰ ìº”ë“¤ì€ ë¯¸ì™„ë£Œì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì œì™¸)
            if len(market_data_list) > 1:
                return market_data_list[:-1]  # ë§ˆì§€ë§‰ ìº”ë“¤ ì œì™¸
            else:
                return market_data_list
                
        except Exception as e:
            logger.error(f"âŒ Failed to fetch realtime data for {symbol}: {e}")
            return []
    
    def get_data(
        self, 
        symbol: str, 
        start_date: datetime, 
        end_date: datetime,
        timeframe: str = None
    ) -> pd.DataFrame:
        """ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ íŠ¹ì • ê¸°ê°„ì˜ ë°ì´í„° ì¡°íšŒ"""
        
        timeframe = timeframe or self.primary_timeframe
        
        query = """
            SELECT 
                time, symbol, timeframe, open, high, low, close, volume,
                quote_volume, is_interpolated, data_source, created_at
            FROM market_data.price_data
            WHERE symbol = %(symbol)s 
                AND timeframe = %(timeframe)s
                AND time BETWEEN %(start_date)s AND %(end_date)s
            ORDER BY time ASC
        """
        
        params = {
            'symbol': symbol,
            'timeframe': timeframe,
            'start_date': start_date,
            'end_date': end_date
        }
        
        try:
            with self.db_manager as db:
                result = db.execute_query(query, params)
                df = pd.DataFrame(result.fetchall())
                
                if not df.empty:
                    df.set_index('time', inplace=True)
                    logger.info(f"ğŸ“Š Retrieved {len(df)} records for {symbol} {timeframe}")
                else:
                    logger.warning(f"âš ï¸ No data found for {symbol} {timeframe} in specified period")
                
                return df
                
        except Exception as e:
            logger.error(f"âŒ Failed to retrieve data for {symbol}: {e}")
            return pd.DataFrame()
    
    async def collect_and_store_data(self, symbols: List[str] = None) -> Dict[str, int]:
        """ë°ì´í„° ìˆ˜ì§‘ ë° ì €ì¥ (ë©”ì¸ ì‘ì—… í•¨ìˆ˜)"""
        
        symbols = symbols or self.target_symbols
        results = {}
        
        logger.info(f"ğŸ”„ Starting data collection for {len(symbols)} symbols...")
        
        for symbol in symbols:
            try:
                # ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘
                market_data_list = await self.fetch_realtime_data(symbol)
                
                if market_data_list:
                    # ë°ì´í„° ê²€ì¦
                    validated_data, quality_metrics = self.validator.validate_data_integrity(market_data_list)
                    self.quality_metrics[symbol] = quality_metrics
                    
                    # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
                    with self.db_manager as db:
                        inserted_count = db.insert_market_data(validated_data)
                        results[symbol] = inserted_count
                        
                        logger.info(f"ğŸ’¾ Stored {inserted_count} records for {symbol} "
                                   f"(Quality: {quality_metrics.quality_score:.3f})")
                else:
                    results[symbol] = 0
                    logger.warning(f"âš ï¸ No new data for {symbol}")
                
            except Exception as e:
                logger.error(f"âŒ Data collection failed for {symbol}: {e}")
                results[symbol] = -1  # ì˜¤ë¥˜ í‘œì‹œ